---
---

@article{papoudakis2019dealing,
  abbr={arXiv},
  title={Dealing with non-stationarity in multi-agent deep reinforcement learning},
  author={Papoudakis, Georgios and Christianos, Filippos and Rahman, Arrasy and Albrecht, Stefano V},
  journal={arXiv preprint arXiv:1906.04737},
  year={2019}
}

@inproceedings{christianos2021scaling,
  abbr={ICML},
  title={Scaling multi-agent reinforcement learning with selective parameter sharing},
  author={Christianos, Filippos and Papoudakis, Georgios and Rahman, Muhammad A and Albrecht, Stefano V},
  booktitle={International Conference on Machine Learning},
  pages={1989--1998},
  year={2021},
  organization={PMLR}
}

@inproceedings{rahman2021towards,
  title={Towards open ad hoc teamwork using graph-based policy learning},
  author={Rahman, Muhammad A and Hopner, Niklas and Christianos, Filippos and Albrecht, Stefano V},
  booktitle={International Conference on Machine Learning},
  pages={8776--8786},
  year={2021},
  organization={PMLR},
  selected={true},
  preview={GPL.png},
  abbr={ICML}
}

@inproceedings{mirsky2022survey,
  abbr={EUMAS},
  title={A survey of ad hoc teamwork research},
  author={Mirsky, Reuth and Carlucho, Ignacio and Rahman, Arrasy and Fosong, Elliot and Macke, William and Sridharan, Mohan and Stone, Peter and Albrecht, Stefano V},
  booktitle={European Conference on Multi-Agent Systems},
  pages={275--293},
  year={2022},
  organization={Springer},
  selected={true}
}

@inproceedings{hanna2021interpretable,
  abbr={IROS},
  title={Interpretable goal recognition in the presence of occluded factors for autonomous vehicles},
  author={Hanna, Josiah P and Rahman, Arrasy and Fosong, Elliot and Eiras, Francisco and Dobre, Mihai and Redford, John and Ramamoorthy, Subramanian and Albrecht, Stefano V},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={7044--7051},
  year={2021},
  organization={IEEE}
}

@article{rahman2023generating,
abbr={TMLR},
title={Generating Teammates for Training Robust Ad Hoc Teamwork Agents via Best-Response Diversity},
author={Arrasy Rahman and Elliot Fosong and Ignacio Carlucho and Stefano V Albrecht},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=l5BzfQhROl},
note={},
selected={true},
preview={BRDiv.png},
abbr={TMLR},
}

@article{ahmed2022deep,
  abbr={AIC},
  title={Deep reinforcement learning for multi-agent interaction},
  author={Ahmed, Ibrahim H and Brewitt, Cillian and Carlucho, Ignacio and Christianos, Filippos and Dunion, Mhairi and Fosong, Elliot and Garcin, Samuel and Guo, Shangmin and Gyevnar, Balint and McInroe, Trevor and others},
  journal={AI Communications},
  volume={35},
  number={4},
  pages={357--368},
  year={2022},
  publisher={IOS Press Nieuwe Hemweg 6B, 1013 BG Amsterdam, The Netherlands}
}

@article{fosong2023learning,
  abbr={arXiv},
  title={Learning Complex Teamwork Tasks using a Sub-task Curriculum},
  author={Fosong, Elliot and Rahman, Arrasy and Carlucho, Ignacio and Albrecht, Stefano V},
  journal={arXiv preprint arXiv:2302.04944},
  year={2023}
}

@article{rahman2022general,
  abbr={arXiv},
  title={A General Learning Framework for Open Ad Hoc Teamwork Using Graph-based Policy Learning},
  author={Rahman, Arrasy and Carlucho, Ignacio and H{\"o}pner, Niklas and Albrecht, Stefano V},
  journal={arXiv preprint arXiv:2210.05448},
  year={2022}
}

@article{rahman2023general,
  abbr={JMLR},
  title={A General Learning Framework for Open Ad Hoc Teamwork Using Graph-based Policy Learning},
  author={Rahman, Arrasy and Carlucho, Ignacio and H{\"o}pner, Niklas and Albrecht, Stefano V},
  journal={Journal of Machine Learning Research},
  year={2023},
  selected={true}
}

@article{rahman2022general,
  abbr={arXiv},
  title={A General Learning Framework for Open Ad Hoc Teamwork Using Graph-based Policy Learning},
  author={Rahman, Arrasy and Carlucho, Ignacio and H{\"o}pner, Niklas and Albrecht, Stefano V},
  journal={arXiv preprint arXiv:2210.05448},
  year={2022},
  selected={true}
}

@article{carlucho2022cooperative,
  abbr={WAHT},
  title={Cooperative marine operations via ad hoc teams},
  author={Carlucho, Ignacio and Rahman, Arrasy and Ard, William and Fosong, Elliot and Barbalata, Corina and Albrecht, Stefano V},
  journal={arXiv preprint arXiv:2207.07498},
  year={2022}
}

@phdthesis{rahman2023advances,
  abbr    = {Thesis},
  author  = {Arrasy Rahman},
  title   = {Advances in open ad hoc teamwork and teammate generation},
  school  = {The University of Edinburgh},
  year    = {2023}
}

@misc{rahman2023minimum,
      abbr={arXiv},
      title={Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents}, 
      author={Arrasy Rahman and Jiaxun Cui and Peter Stone},
      year={2023},
      eprint={2308.09595},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      selected={true}
}

@article{Rahman_Cui_Stone_2024, title={Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/29702}, DOI={10.1609/aaai.v38i16.29702}, 
abstractNote={Robustly cooperating with unseen agents and human partners presents significant challenges due to the diverse cooperative conventions these partners may adopt. Existing Ad Hoc Teamwork (AHT) methods address this 
challenge by training an agent with a population of diverse teammate policies obtained through maximizing specific diversity metrics. However, prior heuristic-based diversity metrics do not always maximize the agent’s robustness in 
all cooperative problems. In this work, we first propose that maximizing an AHT agent’s robustness requires it to emulate policies in the minimum coverage set (MCS), the set of best-response policies to any partner policies in the 
environment. We then introduce the L-BRDiv algorithm that generates a set of teammate policies that, when used for AHT training, encourage agents to emulate policies from the MCS. L-BRDiv works by solving a constrained optimization 
problem to jointly train teammate policies for AHT training and approximating AHT agent policies that are members of the MCS. We empirically demonstrate that L-BRDiv produces more robust AHT agents than state-of-the-art methods in a 
broader range of two-player cooperative problems without the need for extensive hyperparameter tuning for its objectives. Our study shows that L-BRDiv outperforms the baseline methods by prioritizing discovering distinct members of 
the MCS instead of repeatedly finding redundant policies.}, number={16}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Rahman, Muhammad and Cui, Jiaxun and Stone, Peter}, year={2024}, month={Mar.}, 
pages={17523-17530}, selected={true}}

@inproceedings{fosong2024learning,
  title={Learning Complex Teamwork Tasks using a Given Sub-task Curriculum},
  author={Fosong, Elliot and Rahman, Arrasy and Carlucho, Ignacio and Albrecht, Stefano V},
  booktitle={International Conference on Autonomous Agents and Multiagent Systems},
  year={2024}
}
